# -*- coding: utf-8 -*-
"""Different responses for same idea_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14gW0qoE01BZp1RgnOwiphat0f6uB6wZG
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd
import numpy as np
import re
import time
import warnings
from collections import defaultdict
from tqdm import tqdm

# NLP and ML
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering, Birch
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# Embedding
from sentence_transformers import SentenceTransformer

# Visualization
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import seaborn as sns
import matplotlib.gridspec as gridspec
from matplotlib.patches import Polygon
from matplotlib.path import Path
import matplotlib.patches as patches

# Suppress warnings
warnings.filterwarnings('ignore')

# Download necessary NLTK resources
def download_nltk_resources():
    """Download NLTK resources needed for text processing"""
    resources = ['punkt', 'stopwords', 'wordnet']
    for resource in resources:
        try:
            nltk.data.find(f'tokenizers/{resource}')
        except LookupError:
            print(f"Downloading {resource}...")
            nltk.download(resource)

# Enhanced text preprocessing
def preprocess_text(text, lemmatize=True):
    """
    Enhanced text preprocessing with optional word form reduction

    Parameters:
        text (str): Input text
        lemmatize (bool): Whether to apply lemmatization

    Returns:
        str: Preprocessed text
    """
    if not isinstance(text, str):
        return ""

    # Extended contractions handling
    text = (text.replace("don't", "do not")
                .replace("can't", "cannot")
                .replace("won't", "will not")
                .replace("I'm", "I am")
                .replace("isn't", "is not")
                .replace("aren't", "are not"))

    # Remove special characters but retain sentence structure
    text = re.sub(r'[^\w\s\.\,\?\!]', '', text)

    # Convert to lowercase
    text = text.lower()

    # Tokenize
    words = word_tokenize(text)

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]

    # Lemmatize if requested
    if lemmatize:
        lemmatizer = WordNetLemmatizer()
        words = [lemmatizer.lemmatize(word) for word in words]

    # Join words back into text
    preprocessed_text = ' '.join(words)

    return preprocessed_text

# Extractive summarization based on sentence embeddings
def extractive_summarize(texts, num_sentences=3, embedding_model="paraphrase-MiniLM-L6-v2"):
    """
    Extractive summarization based on sentence embeddings

    Args:
        texts (list): List of text documents
        num_sentences (int): Number of sentences to include in the summary
        embedding_model (str): Sentence Transformer model name

    Returns:
        str: Summary text
    """
    # Extract sentences from all texts
    all_sentences = []
    for text in texts:
        cleaned = preprocess_text(text, lemmatize=False)
        all_sentences.extend(sent_tokenize(cleaned))

    # Remove duplicate sentences
    sentences = list(set(all_sentences))

    if len(sentences) <= num_sentences:
        return ' '.join(sentences)

    # Load sentence embedding model
    model = SentenceTransformer(embedding_model)

    # Calculate sentence embeddings
    sentence_embeddings = model.encode(sentences)

    # Calculate centroid of all embeddings
    centroid = np.mean(sentence_embeddings, axis=0)

    # Calculate similarity to centroid
    similarities = np.dot(sentence_embeddings, centroid) / (
        np.linalg.norm(sentence_embeddings, axis=1) * np.linalg.norm(centroid)
    )

    # Select most similar sentences
    top_indices = np.argsort(similarities)[-num_sentences:]

    # Sort indices in original order
    top_indices = sorted(top_indices)

    # Combine sentences into summary
    summary = ' '.join([sentences[i] for i in top_indices])

    return summary

def textrank_summarize(texts, num_sentences=3):
    """
    Generate summary using TextRank algorithm

    Args:
        texts (list): List of text documents
        num_sentences (int): Number of sentences to include in the summary

    Returns:
        str: Summary text
    """
    # Combine all texts
    combined_text = ' '.join(texts)

    try:
        # Process text with spaCy
        import spacy
        try:
            nlp = spacy.load('en_core_web_sm')
        except:
            # Download model if not available
            import sys
            import subprocess
            subprocess.check_call([sys.executable, "-m", "spacy", "download", "en_core_web_sm"])
            nlp = spacy.load('en_core_web_sm')

        doc = nlp(combined_text)

        # Extract sentences
        sentences = [sent.text for sent in doc.sents]

        if len(sentences) <= num_sentences:
            return combined_text

        # Create sentence vectors
        sentence_vectors = []
        for sent in sentences:
            sent_doc = nlp(sent)
            sentence_vectors.append(sent_doc.vector)

        # Create similarity matrix
        similarity_matrix = np.zeros([len(sentences), len(sentences)])
        for i in range(len(sentences)):
            for j in range(len(sentences)):
                if i != j:
                    similarity_matrix[i][j] = cosine_similarity(
                        [sentence_vectors[i]], [sentence_vectors[j]]
                    )[0, 0]

        # Calculate sentence scores using PageRank-like algorithm
        scores = np.array([1.0] * len(sentences))
        damping = 0.85
        iterations = 10

        for _ in range(iterations):
            new_scores = np.array([1 - damping] * len(sentences))
            for i in range(len(sentences)):
                for j in range(len(sentences)):
                    if i != j and similarity_matrix[j][i] > 0:
                        new_scores[i] += damping * scores[j] * similarity_matrix[j][i] / \
                                       np.sum(similarity_matrix[j])
            scores = new_scores

        # Select sentences with highest scores
        ranked_indices = np.argsort(scores)[::-1]
        top_indices = ranked_indices[:num_sentences]

        # Sort indices in original order
        top_indices = sorted(top_indices)

        # Combine sentences into summary
        summary = ' '.join([sentences[i] for i in top_indices])

        return summary
    except Exception as e:
        print(f"TextRank summarization failed: {e}")
        # Fallback to extractive summarization
        return extractive_summarize(texts, num_sentences)

def compare_summary_methods(texts, num_sentences=3):
    """
    Compare different summarization methods and select the best one

    Args:
        texts (list): List of text documents
        num_sentences (int): Number of sentences to include in the summary

    Returns:
        dict: Dictionary containing summaries from different methods and the best summary
    """
    # Generate summaries using different methods
    extractive_summary = extractive_summarize(texts, num_sentences)

    try:
        textrank_summary = textrank_summarize(texts, num_sentences)
    except:
        textrank_summary = extractive_summary

    # In this implementation, we use extractive as the default best method
    # In a more sophisticated approach, we could evaluate summary quality

    return {
        'extractive': extractive_summary,
        'textrank': textrank_summary,
        'best': extractive_summary
    }

# Get text embeddings using Sentence Transformer
def get_text_embeddings(texts, model_name="paraphrase-MiniLM-L6-v2"):
    """
    Get text embeddings using Sentence Transformer

    Parameters:
        texts (list): List of text documents
        model_name (str): Sentence Transformer model name

    Returns:
        np.array: Text embedding matrix
    """
    model = SentenceTransformer(model_name)
    embeddings = model.encode(texts)
    return embeddings

# Get TF-IDF vectors
def get_tfidf_vectors(texts):
    """
    Get TF-IDF vectors for a list of texts

    Parameters:
        texts (list): List of text documents

    Returns:
        scipy.sparse.csr.csr_matrix: TF-IDF matrix
    """
    vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
    return vectorizer.fit_transform(texts)

# Get available clustering algorithms
def get_clustering_algorithms():
    """
    Get dictionary of available clustering algorithms

    Returns:
        dict: Dictionary mapping algorithm names to initializer functions
    """
    clustering_algorithms = {
        'KMeans': lambda n_clusters: KMeans(n_clusters=n_clusters, random_state=42, n_init=10),
        'AgglomerativeClustering': lambda n_clusters: AgglomerativeClustering(n_clusters=n_clusters),
        'SpectralClustering': lambda n_clusters: SpectralClustering(n_clusters=n_clusters,
                                                                   assign_labels='discretize',
                                                                   random_state=42),
        'GaussianMixture': lambda n_clusters: GaussianMixture(n_components=n_clusters,
                                                           random_state=42),
        'Birch': lambda n_clusters: Birch(n_clusters=n_clusters)
    }

    # Try to add HDBSCAN if available
    try:
        from hdbscan import HDBSCAN
        clustering_algorithms['HDBSCAN'] = lambda n_clusters: HDBSCAN(min_cluster_size=2,
                                                                     min_samples=1,
                                                                     prediction_data=True)
    except ImportError:
        pass

    return clustering_algorithms

# Evaluate clustering results using multiple metrics
def evaluate_clustering(X, labels):
    """
    Evaluate clustering results using multiple metrics

    Parameters:
        X (array-like): Data matrix
        labels (array-like): Cluster labels

    Returns:
        dict: Dictionary of evaluation metrics
    """
    # Number of unique clusters
    n_clusters = len(np.unique(labels))

    # Initial metrics dictionary
    metrics = {
        'n_clusters': n_clusters,
        'silhouette': -1,
        'davies_bouldin': float('inf'),
        'calinski_harabasz': 0
    }

    # If only one cluster, most metrics can't be calculated
    if n_clusters <= 1:
        return metrics

    # Calculate silhouette score (higher is better)
    try:
        metrics['silhouette'] = silhouette_score(X, labels)
    except:
        pass

    # Calculate Davies-Bouldin index (lower is better)
    try:
        metrics['davies_bouldin'] = davies_bouldin_score(X, labels)
    except:
        pass

    # Calculate Calinski-Harabasz index (higher is better)
    try:
        metrics['calinski_harabasz'] = calinski_harabasz_score(X, labels)
    except:
        pass

    return metrics

# Find optimal number of clusters
def find_optimal_clusters(X, max_clusters=7, min_clusters=2):
    """
    Find the optimal number of clusters using multiple evaluation metrics

    Parameters:
        X (array-like): Data matrix
        max_clusters (int): Maximum number of clusters to try
        min_clusters (int): Minimum number of clusters to try

    Returns:
        int: Optimal number of clusters
    """
    # Handle small datasets
    if X.shape[0] < min_clusters:
        return min(2, X.shape[0])

    # Adjust max_clusters based on data size
    max_clusters = min(max_clusters, X.shape[0] // 2 + 1)
    min_clusters = min(min_clusters, X.shape[0] - 1)

    # If data is too small or min_clusters > max_clusters
    if X.shape[0] <= 3 or min_clusters >= max_clusters:
        return min(2, X.shape[0])

    # Tracking metrics for each k
    silhouette_scores = []
    davies_bouldin_scores = []
    calinski_harabasz_scores = []

    k_range = range(min_clusters, max_clusters + 1)

    for k in k_range:
        # Skip if k is too large for the data
        if k >= X.shape[0]:
            silhouette_scores.append(0)
            davies_bouldin_scores.append(float('inf'))
            calinski_harabasz_scores.append(0)
            continue

        try:
            # Use KMeans for evaluation
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(X)

            # Skip if not enough clusters formed
            if len(np.unique(labels)) < min_clusters:
                silhouette_scores.append(0)
                davies_bouldin_scores.append(float('inf'))
                calinski_harabasz_scores.append(0)
                continue

            # Calculate evaluation metrics
            try:
                silhouette_scores.append(silhouette_score(X, labels))
            except:
                silhouette_scores.append(0)

            try:
                davies_bouldin_scores.append(davies_bouldin_score(X, labels))
            except:
                davies_bouldin_scores.append(float('inf'))

            try:
                calinski_harabasz_scores.append(calinski_harabasz_score(X, labels))
            except:
                calinski_harabasz_scores.append(0)

        except Exception as e:
            print(f"Error calculating metrics for k={k}: {str(e)}")
            silhouette_scores.append(0)
            davies_bouldin_scores.append(float('inf'))
            calinski_harabasz_scores.append(0)

    # Handle case where all scores are zero
    if len(silhouette_scores) == 0 or max(silhouette_scores) == 0:
        return min(3, X.shape[0])

    try:
        # Normalize scores for comparison
        # Silhouette: higher is better (normalize to 0-1)
        if max(silhouette_scores) > 0:
            norm_silhouette = np.array(silhouette_scores) / max(silhouette_scores)
        else:
            norm_silhouette = np.zeros_like(silhouette_scores)

        # Davies-Bouldin: lower is better (invert and normalize to 0-1)
        if min(davies_bouldin_scores) < float('inf'):
            min_valid_db = min([score for score in davies_bouldin_scores if score < float('inf')])
            max_valid_db = max([score for score in davies_bouldin_scores if score < float('inf')])
            if max_valid_db > min_valid_db:
                norm_davies = 1 - ((np.array(davies_bouldin_scores) - min_valid_db) / (max_valid_db - min_valid_db))
                norm_davies[np.isinf(davies_bouldin_scores)] = 0
            else:
                norm_davies = np.zeros_like(davies_bouldin_scores)
        else:
            norm_davies = np.zeros_like(davies_bouldin_scores)

        # Calinski-Harabasz: higher is better (normalize to 0-1)
        if max(calinski_harabasz_scores) > 0:
            norm_calinski = np.array(calinski_harabasz_scores) / max(calinski_harabasz_scores)
        else:
            norm_calinski = np.zeros_like(calinski_harabasz_scores)

        # Combine scores (higher is better)
        combined_scores = norm_silhouette + norm_davies + norm_calinski

        # Get optimal number of clusters
        best_k_idx = np.argmax(combined_scores)
        best_k = list(k_range)[best_k_idx]

    except Exception as e:
        print(f"Error normalizing scores: {str(e)}")
        if max(silhouette_scores) > 0:
            best_k_idx = np.argmax(silhouette_scores)
            best_k = list(k_range)[best_k_idx]
        else:
            best_k = min(3, X.shape[0])

    return best_k

# Compare different clustering algorithms
def compare_clustering_algorithms(X, n_clusters, algorithms=None):
    """
    Compare the performance of different clustering algorithms

    Parameters:
        X (array-like): Data matrix
        n_clusters (int): Number of clusters
        algorithms (dict): Dictionary of clustering algorithms

    Returns:
        dict: Results for each algorithm
    """
    if algorithms is None:
        algorithms = get_clustering_algorithms()

    results = {}

    for name, algo_func in algorithms.items():
        try:
            # Initialize and fit the algorithm
            clustering_algo = algo_func(n_clusters)
            labels = clustering_algo.fit_predict(X)

            # Evaluate clustering
            metrics = evaluate_clustering(X, labels)

            # Count actual number of clusters formed
            actual_clusters = len(np.unique(labels[labels >= 0]))  # Consider noise points (-1)

            results[name] = {
                'labels': labels,
                'metrics': metrics,
                'actual_clusters': actual_clusters
            }

        except Exception as e:
            print(f"Algorithm {name} encountered an error: {str(e)}")
            continue

    return results

# Select the best clustering algorithm
def select_best_algorithm(clustering_results):
    """
    Select the best clustering algorithm based on evaluation metrics

    Parameters:
        clustering_results (dict): Results from compare_clustering_algorithms

    Returns:
        tuple: (Best algorithm name, Cluster labels, Algorithm scores)
    """
    if not clustering_results:
        return None, None, {}

    # Calculate combined score for each algorithm
    scores = {}

    for name, result in clustering_results.items():
        metrics = result['metrics']
        actual_clusters = result['actual_clusters']

        # Skip algorithms that didn't produce multiple clusters
        if actual_clusters < 1:
            continue

        # Calculate combined score - weighted sum of metrics
        # Higher silhouette and calinski_harabasz are better, lower davies_bouldin is better
        silhouette_weight = 0.5  # Silhouette is most important
        davies_weight = 0.3      # Davies-Bouldin is next
        calinski_weight = 0.2    # Calinski-Harabasz is still useful

        # Normalize and combine scores
        # Silhouette: -1 to 1, higher is better
        norm_silhouette = (metrics['silhouette'] + 1) / 2  # Normalize to 0-1

        # Davies-Bouldin: 0 to infinity, lower is better
        norm_davies = 1 / (1 + metrics['davies_bouldin'])  # Normalize to 0-1

        # Calinski-Harabasz: 0 to infinity, higher is better
        # Use log to handle large values
        if metrics['calinski_harabasz'] > 0:
            norm_calinski = min(1, np.log1p(metrics['calinski_harabasz']) / 10)
        else:
            norm_calinski = 0

        # Combined score
        score = (silhouette_weight * norm_silhouette +
                 davies_weight * norm_davies +
                 calinski_weight * norm_calinski)

        scores[name] = {
            'combined_score': score,
            'norm_silhouette': norm_silhouette,
            'norm_davies': norm_davies,
            'norm_calinski': norm_calinski,
            'silhouette': metrics['silhouette'],
            'davies_bouldin': metrics['davies_bouldin'],
            'calinski_harabasz': metrics['calinski_harabasz']
        }

    if not scores:
        return None, None, {}

    # Select algorithm with highest score
    best_algorithm = max(scores.items(), key=lambda x: x[1]['combined_score'])[0]
    best_labels = clustering_results[best_algorithm]['labels']

    return best_algorithm, best_labels, scores

# NEW FUNCTION: Visualize algorithm comparison
def visualize_algorithm_comparison(clustering_results, scores, category_name, output_dir=None):
    """
    Visualize comparison of different clustering algorithms' performance

    Parameters:
        clustering_results (dict): Results from compare_clustering_algorithms
        scores (dict): Algorithm scores from select_best_algorithm
        category_name (str): Category name for title
        output_dir (str): Directory to save visualizations
    """
    if not scores:
        print(f"No valid scores for category '{category_name}'. Skipping visualization.")
        return

    # Create a safe category name for file saving
    safe_category = re.sub(r'[^\w\-]', '_', category_name)

    # Get algorithm names and their combined scores
    algorithms = list(scores.keys())
    combined_scores = [scores[algo]['combined_score'] for algo in algorithms]

    # Sort algorithms by their combined scores (descending)
    sorted_indices = np.argsort(combined_scores)[::-1]
    sorted_algorithms = [algorithms[i] for i in sorted_indices]
    sorted_scores = [combined_scores[i] for i in sorted_indices]

    # Highlight the best algorithm
    best_algorithm = sorted_algorithms[0]
    colors = ['#3498db' if algo != best_algorithm else '#e74c3c' for algo in sorted_algorithms]

    # Create a bar chart comparing combined scores
    plt.figure(figsize=(12, 8))

    # Create a GridSpec to layout the plots
    gs = gridspec.GridSpec(2, 2, height_ratios=[2, 1], width_ratios=[3, 1])

    # Combined score bar chart
    ax1 = plt.subplot(gs[0, 0])  # Top left
    bars = ax1.bar(sorted_algorithms, sorted_scores, color=colors)
    ax1.set_title(f'Algorithm Comparison for Category: "{category_name}"', fontsize=14)
    ax1.set_xlabel('Clustering Algorithm')
    ax1.set_ylabel('Combined Performance Score (higher is better)')
    ax1.set_ylim(0, max(combined_scores) * 1.2)  # Add some space above bars

    # Add value labels on top of the bars
    for i, bar in enumerate(bars):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{sorted_scores[i]:.3f}', ha='center', va='bottom', fontsize=10)

    # Add "BEST" label for the highest bar
    ax1.text(0, sorted_scores[0] + 0.05, 'BEST', ha='center', va='bottom',
             fontsize=12, color='darkred', fontweight='bold')

    # Add a detailed metrics comparison table
    ax2 = plt.subplot(gs[0, 1])  # Top right
    ax2.axis('off')  # Hide axis

    # Create table data
    metric_names = ['Silhouette', 'Davies-Bouldin', 'Calinski-Harabasz', 'Combined']
    table_data = []

    for algo in sorted_algorithms:
        row = [
            f"{scores[algo]['silhouette']:.3f}",
            f"{scores[algo]['davies_bouldin']:.3f}",
            f"{scores[algo]['calinski_harabasz']:.1f}",
            f"{scores[algo]['combined_score']:.3f}"
        ]
        table_data.append(row)

    # Create table with bold text for the best algorithm
    the_table = ax2.table(
        cellText=table_data,
        rowLabels=sorted_algorithms,
        colLabels=metric_names,
        cellLoc='center',
        rowLoc='center',
        loc='center'
    )

    # Style the table
    the_table.auto_set_font_size(False)
    the_table.set_fontsize(10)
    the_table.scale(1, 1.5)

    # Highlight the best algorithm row
    for i in range(len(metric_names)):
        cell = the_table[1, i]
        cell.set_text_props(fontweight='bold', color='darkred')

    # Add explanation text
    ax3 = plt.subplot(gs[1, :])  # Bottom full width
    ax3.axis('off')  # Hide axis

    explanation_text = f"""
    The performance of different clustering algorithms was evaluated for the category '{category_name}'.

    Metrics used:
    - Silhouette Score: Measures how similar points are to their own cluster vs other clusters (range -1 to 1, higher is better)
    - Davies-Bouldin Index: Measures cluster separation (lower is better)
    - Calinski-Harabasz Index: Measures cluster density and separation (higher is better)

    The best performing algorithm is {best_algorithm} with a combined score of {scores[best_algorithm]['combined_score']:.3f}.
    This algorithm was selected for clustering the texts in this category based on its superior performance across the metrics.
    """

    ax3.text(0.5, 0.5, explanation_text, ha='center', va='center',
             fontsize=11, transform=ax3.transAxes,
             bbox=dict(boxstyle="round,pad=1", fc="#f8f9fa", ec="#d6d6d6", alpha=0.9))

    plt.tight_layout()

    # Save or show the plot
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        filename = f"{output_dir}/{safe_category}_algorithm_comparison.png"
        plt.savefig(filename, dpi=150, bbox_inches='tight')
        plt.close()
        return filename
    else:
        plt.show()
        return None

# Visualize clustering metrics
def visualize_clustering_metrics(category, k_range, silhouette_scores, davies_bouldin_scores, calinski_harabasz_scores, output_dir=None):
    """
    Visualize clustering metrics for different number of clusters

    Parameters:
        category (str): Category name for title
        k_range (list): Range of cluster numbers
        silhouette_scores (list): Silhouette scores for each k
        davies_bouldin_scores (list): Davies-Bouldin scores for each k
        calinski_harabasz_scores (list): Calinski-Harabasz scores for each k
        output_dir (str): Directory to save plots (if None, plots are shown)
    """
    plt.figure(figsize=(15, 5))

    # Normalize metrics for comparison
    norm_silhouette = np.array(silhouette_scores) / max(silhouette_scores) if max(silhouette_scores) > 0 else silhouette_scores
    norm_davies = 1 - (np.array(davies_bouldin_scores) / max(davies_bouldin_scores)) if max(davies_bouldin_scores) > 0 else davies_bouldin_scores
    norm_calinski = np.array(calinski_harabasz_scores) / max(calinski_harabasz_scores) if max(calinski_harabasz_scores) > 0 else calinski_harabasz_scores

    # Plot silhouette score
    plt.subplot(1, 3, 1)
    plt.plot(k_range, silhouette_scores, 'o-', color='blue')
    plt.title('Silhouette Score (higher is better)')
    plt.xlabel('Number of Clusters')
    plt.ylabel('Score')
    plt.grid(True)

    # Plot Davies-Bouldin index
    plt.subplot(1, 3, 2)
    plt.plot(k_range, davies_bouldin_scores, 'o-', color='red')
    plt.title('Davies-Bouldin Index (lower is better)')
    plt.xlabel('Number of Clusters')
    plt.ylabel('Score')
    plt.grid(True)

    # Plot Calinski-Harabasz index
    plt.subplot(1, 3, 3)
    plt.plot(k_range, calinski_harabasz_scores, 'o-', color='green')
    plt.title('Calinski-Harabasz Index (higher is better)')
    plt.xlabel('Number of Clusters')
    plt.ylabel('Score')
    plt.grid(True)

    plt.tight_layout()
    plt.suptitle(f'Clustering Metrics for Category "{category}"', fontsize=16)
    plt.subplots_adjust(top=0.85)

    # Save or show the plot
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        safe_category = re.sub(r'[^\w\-]', '_', category)
        filename = f"{output_dir}/{safe_category}_metrics.png"
        plt.savefig(filename, dpi=150)
        plt.close()
    else:
        plt.show()

    # Plot combined score
    plt.figure(figsize=(10, 5))
    combined_scores = norm_silhouette + norm_davies + norm_calinski

    plt.plot(k_range, combined_scores, 'o-', color='purple')
    plt.title(f'Combined Score for Category "{category}" (higher is better)')
    plt.xlabel('Number of Clusters')
    plt.ylabel('Normalized Combined Score')
    plt.grid(True)

    # Save or show the plot
    if output_dir:
        safe_category = re.sub(r'[^\w\-]', '_', category)
        filename = f"{output_dir}/{safe_category}_combined_score.png"
        plt.savefig(filename, dpi=150)
        plt.close()
    else:
        plt.show()

    # Find and print optimal number of clusters
    best_k_idx = np.argmax(combined_scores)
    best_k = k_range[best_k_idx]
    print(f'Optimal number of clusters: {best_k}')

    return best_k

# Visualize clusters in 2D
def visualize_clusters(X, labels, algorithm_name, category_name, output_dir=None):
    """
    Visualize clusters using dimensionality reduction

    Parameters:
        X (array-like): Data matrix
        labels (array-like): Cluster labels
        algorithm_name (str): Name of the clustering algorithm
        category_name (str): Name of the category
        output_dir (str): Directory to save plots (if None, plots are shown)
    """
    n_samples = X.shape[0]

    if n_samples < 5:
        print(f"Too few samples ({n_samples}) for t-SNE visualization. Skipping.")
        return

    # Adjust perplexity for small datasets
    perplexity = min(30, n_samples - 1)

    try:
        # Use t-SNE for visualization
        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)
        X_tsne = tsne.fit_transform(X)

        plt.figure(figsize=(10, 8))
        unique_labels = np.unique(labels)

        # Generate colors for clusters
        colors = cm.rainbow(np.linspace(0, 1, len(unique_labels)))

        # Plot each cluster
        for i, label in enumerate(unique_labels):
            if label == -1:
                color = 'black'  # Noise points in black
            else:
                color = colors[i]

            plt.scatter(X_tsne[labels == label, 0],
                       X_tsne[labels == label, 1],
                       s=50,
                       color=color,
                       label=f'Cluster {label} ({np.sum(labels == label)} items)')

        plt.title(f'{algorithm_name} Clustering for Category "{category_name}"')
        plt.legend()
        plt.grid(True, alpha=0.3)

        # Save or show the plot
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            safe_category = re.sub(r'[^\w\-]', '_', category_name)
            filename = f"{output_dir}/{safe_category}_{algorithm_name}_clusters.png"
            plt.savefig(filename, dpi=150)
            plt.close()
        else:
            plt.show()

    except Exception as e:
        print(f"Error during t-SNE visualization: {str(e)}")
        print(f"Number of samples: {n_samples}, perplexity: {perplexity}")
        print("Skipping visualization due to error.")

# Process a single category
def process_category(category_data, visualize=True, output_dir=None):
    """
    Process a single category of data

    Parameters:
        category_data (pandas.DataFrame): Data for one category
        visualize (bool): Whether to generate visualizations
        output_dir (str): Directory to save visualizations

    Returns:
        list: Processed category results
    """
    category_name = category_data['Category'].iloc[0]

    # Handle very small datasets
    if category_data.shape[0] <= 2:
        # Create a single group for the entire category
        group_name = re.sub(r'[^A-Za-z]', '', category_name)[:3] + "0"

        # Generate summary for small category
        responses = category_data['Response'].tolist()
        summary_info = compare_summary_methods(responses)
        summary = summary_info['best']

        # Create results
        result = []
        for _, row in category_data.iterrows():
            result.append({
                'Category': category_name,
                'Groups': group_name,
                'Summary': summary,  # Add summary
                'Response': row['Response'],
                'Upvotes': row.get('Upvotes', 0),
                'Downvotes': row.get('Downvotes', 0),
                'Open Location Code': row.get('Open Location Code', ''),
                'geometry': row.get('geometry', '')
            })

        return result

    # Preprocess texts
    preprocessed_texts = [preprocess_text(text) for text in category_data['Response']]

    print(f"Generating text embeddings for category '{category_name}'...")
    embeddings = get_text_embeddings(preprocessed_texts)

    # Find optimal number of clusters
    max_clusters = min(7, category_data.shape[0])
    min_clusters = min(2, category_data.shape[0])

    if visualize:
        # Calculate metrics for different numbers of clusters
        silhouette_scores = []
        davies_bouldin_scores = []
        calinski_harabasz_scores = []
        k_range = range(min_clusters, max_clusters + 1)

        for k in k_range:
            # Skip if k is too large
            if k >= category_data.shape[0]:
                continue

            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(embeddings)

            metrics = evaluate_clustering(embeddings, labels)
            silhouette_scores.append(metrics['silhouette'])
            davies_bouldin_scores.append(metrics['davies_bouldin'])
            calinski_harabasz_scores.append(metrics['calinski_harabasz'])

        # Visualize evaluation metrics
        try:
            visualize_clustering_metrics(
                category_name,
                list(k_range),
                silhouette_scores,
                davies_bouldin_scores,
                calinski_harabasz_scores,
                output_dir
            )
        except Exception as e:
            print(f"Error visualizing metrics: {str(e)}")

    # Find optimal number of clusters
    best_k = find_optimal_clusters(embeddings, max_clusters, min_clusters)
    print(f"Optimal number of clusters for category '{category_name}': {best_k}")

    # Compare different clustering algorithms
    clustering_results = compare_clustering_algorithms(embeddings, best_k)

    # Select the best algorithm
    best_algorithm, best_labels, algorithm_scores = select_best_algorithm(clustering_results)

    # Visualize algorithm comparison (NEW)
    if visualize and algorithm_scores:
        try:
            visualize_algorithm_comparison(
                clustering_results,
                algorithm_scores,
                category_name,
                output_dir
            )
        except Exception as e:
            print(f"Error visualizing algorithm comparison: {str(e)}")

    if best_algorithm is None:
        print(f"Warning: Could not find suitable clustering algorithm for category '{category_name}', using KMeans as fallback")
        kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
        best_labels = kmeans.fit_predict(embeddings)
        best_algorithm = 'KMeans'

    print(f"Selected best algorithm: {best_algorithm}")

    # Visualize clusters
    if visualize:
        visualize_clusters(
            embeddings,
            best_labels,
            best_algorithm,
            category_name,
            output_dir
        )

    # Add cluster labels to the data
    category_data_with_labels = category_data.copy()
    category_data_with_labels['Topic'] = best_labels

    # Process each cluster
    summary_results = []
    category_abbr = re.sub(r'[^A-Za-z]', '', category_name)[:3]

    for topic in np.unique(best_labels):
        if topic == -1 and sum(best_labels == -1) == 0:
            continue  # Skip empty noise cluster

        topic_data = category_data_with_labels[category_data_with_labels['Topic'] == topic]
        topic_responses = topic_data['Response'].tolist()

        # Generate summary for this cluster
        summary_info = compare_summary_methods(topic_responses)
        summary = summary_info['best']

        group_name = f"{category_abbr}{topic}"

        # Add each response in this cluster to the results
        for _, row in topic_data.iterrows():
            summary_results.append({
                'Category': category_name,
                'Groups': group_name,
                'Summary': summary,  # Add summary to results
                'Response': row['Response'],
                'Upvotes': row.get('Upvotes', 0),
                'Downvotes': row.get('Downvotes', 0),
                'Algorithm': best_algorithm,
                'Cluster_Count': best_k,
                'Open Location Code': row.get('Open Location Code', ''),
                'geometry': row.get('geometry', '')
            })

    return summary_results

# Main function
def main(input_path, output_path, visualize=True, output_dir='visualizations'):
    """
    Main function to execute the entire clustering process

    Parameters:
        input_path (str): Path to input CSV file
        output_path (str): Path to output CSV file
        visualize (bool): Whether to generate visualizations
        output_dir (str): Directory to save visualizations

    Returns:
        pandas.DataFrame: Processed data
    """
    start_time = time.time()

    # Create output directory if needed
    if visualize and output_dir:
        os.makedirs(output_dir, exist_ok=True)
        for subdir in ['algorithm_comparison', 'cluster_metrics', 'clusters']:
            os.makedirs(f"{output_dir}/{subdir}", exist_ok=True)

    # Download NLTK resources
    download_nltk_resources()

    print(f"Reading data from: {input_path}")

    try:
        data = pd.read_csv(input_path)
        data.dropna(subset=['Response', 'Category'], inplace=True)

        # Clean OLC data if available
        if 'Open Location Code' in data.columns:
            data['Open Location Code'] = data['Open Location Code'].astype(str)

        print(f"Number of rows: {data.shape[0]}")
        print(f"Number of categories: {data['Category'].nunique()}")
        print(f"Category list: {data['Category'].unique()}")

        final_results = []

        # Process each category
        for category in tqdm(data['Category'].unique(), desc="Processing categories"):
            print(f"\nProcessing category: {category}")
            category_data = data[data['Category'] == category]

            try:
                category_results = process_category(
                    category_data,
                    visualize=visualize,
                    output_dir=output_dir
                )
                final_results.extend(category_results)
            except Exception as e:
                print(f"Error processing category {category}: {str(e)}")
                continue

        # Create final DataFrame
        final_df = pd.DataFrame(final_results)

        # Save to CSV
        final_df.to_csv(output_path, index=False)

        elapsed_time = time.time() - start_time
        print(f"\nProcessing completed in {elapsed_time:.2f} seconds")
        print(f"Results saved to: {output_path}")

        # Print algorithm statistics
        if 'Algorithm' in final_df.columns:
            algo_stats = final_df['Algorithm'].value_counts()
            print("\nAlgorithm selection statistics:")
            print(algo_stats)

        # Print cluster count statistics
        if 'Cluster_Count' in final_df.columns:
            cluster_stats = final_df['Cluster_Count'].value_counts()
            print("\nCluster count statistics:")
            print(cluster_stats)

        return final_df

    except Exception as e:
        print(f"Error in main function: {str(e)}")
        raise

if __name__ == "__main__":
    # Update these paths for your environment
    input_path = './drive/MyDrive/apply_data_science/survey_data/data_geo.csv'
    output_path = './drive/MyDrive/apply_data_science/survey_data/enhanced_clustering_results.csv'
    output_dir = './drive/MyDrive/apply_data_science/survey_data/visualizations'

    result_df = main(input_path, output_path, visualize=True, output_dir=output_dir)

# if __name__ == "__main__":
#     # Update these paths to match your environment
#     input_file = "./drive/MyDrive/apply_data_science/survey_data/data_geo.csv"
#     output_file = "./drive/MyDrive/apply_data_science/survey_data/enhanced_clustering_results.csv"

#     # Run the clustering process
#     results = main(input_file, output_file)

df=pd.read_csv("./drive/MyDrive/apply_data_science/survey_data/enhanced_clustering_results.csv")

df